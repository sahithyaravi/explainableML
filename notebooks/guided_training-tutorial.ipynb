{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided Annotation tool\n",
    "This notebook shows you the whole process of preparing the data which is used as input in the guided annotation tool.\n",
    "The tool basically shows unlabelled data in the form of explainable clusters to label.\n",
    "It will show you the following steps:\n",
    "\n",
    "    1. Load dataset\n",
    "    2. Train a model and explain it\n",
    "    3. Perform shap clustering\n",
    "    4. Save the clusters to database with keywords to be highlighted by the annotation tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, homogeneity_score, v_measure_score, completeness_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "# import chart_studio.plotly as py\n",
    "import seaborn as sns\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in function usage\n",
    "To avoid re-writing a lot of stuff for each dataset/model, I have created some functions in the models module/folder.\n",
    "We are going to use this python module in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('../')\n",
    "from models.trainers import Trainer\n",
    "from app.utils import clear_labels\n",
    "from models.guided_learning import GuidedLearner\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/davidson_dataset.csv') # substitute other datasets in similar format\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data\n",
    "  We split data into training, test, pool and individual. Pool is the unlabelled pool we want to generate SHAP clusters for.\n",
    "  Individual is the bunch of labels we want to get from the user without any guidance\n",
    "  \n",
    "  We split as follows: (can be altered)\n",
    "  70% train\n",
    "  10% test\n",
    "  10% pool\n",
    "  10% individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Trainer(dataset_name=\"davidson\") # the name which you want for the tables in the database\n",
    "df_train, df_test, df_pool, df_individual = t.train_test_pool_split(df)\n",
    "df_train.shape, df_test.shape, df_pool.shape, df_individual.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = GuidedLearner(df_train, df_test, df_pool, df_individual, 'davidson', 1)\n",
    "tfid, x_train, x_test, x_pool, y_train, y_test, y_pool = learner.tfid_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, explainer = learner.grid_search_fit_svc(c=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform shap clustering\n",
    "We are going to cluster the training data using SHAP explanations (shapely space)\n",
    "SHAP clustering works by clustering on Shapley values of each instance. \n",
    "This means that you cluster instances by explanation similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_labels, uncertainty = learner.cluster_data_pool(n_clusters=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert predict probability to uncertainty. In binary classification this would be the same as 1-P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_to_db(df_final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(uncertainty)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional explanations\n",
    "\n",
    "With saving to database, all your steps for the guided annotations are complete.\n",
    "In this section, we show you how to look at explanations of a single instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_train = explainer.shap_values(x_train)\n",
    "shap_values_pool = explainer.shap_values(x_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain a single positive prediction at 'index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postive_index = 0\n",
    "index = np.where(predictions==1)[0][postive_index]\n",
    "print(\"text \", df_test[\"text\"].values[index], \" prediction: \", predictions[index], \"actual \", y_test[index])\n",
    "shap.force_plot(explainer.expected_value, \n",
    "                               shap_values_pool[index,:], \n",
    "                               x_test[index,:], feature_names = tfid.get_feature_names(),\n",
    "               matplotlib=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s164255\\anaconda3\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning:\n",
      "\n",
      "urllib3 (1.26.3) or chardet (4.0.0) doesn't match a supported version!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, homogeneity_score, v_measure_score, completeness_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import chart_studio.plotly as py\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotated_data(params):\n",
    "    #temp_read = pd.read_pickle(params['data_file'])\n",
    "    with open(params['data_file'], 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "    dict_data=[]\n",
    "    for key in data:\n",
    "        temp={}\n",
    "        temp['post_id']=key\n",
    "        temp['text']=data[key]['post_tokens']\n",
    "        final_label=[]\n",
    "        for i in range(1,4):\n",
    "            temp['annotatorid'+str(i)]=data[key]['annotators'][i-1]['annotator_id']\n",
    "#             temp['explain'+str(i)]=data[key]['annotators'][i-1]['rationales']\n",
    "            temp['target'+str(i)]=data[key]['annotators'][i-1]['target']\n",
    "            temp['label'+str(i)]=data[key]['annotators'][i-1]['label']\n",
    "            final_label.append(temp['label'+str(i)])\n",
    "\n",
    "        final_label_id=max(final_label,key=final_label.count)\n",
    "        temp['rationales']=data[key]['rationales']\n",
    "            \n",
    "        if(params['class_names']=='Data/classes_two.npy'):\n",
    "            if(final_label.count(final_label_id)==1):\n",
    "                temp['final_label']='undecided'\n",
    "            else:\n",
    "                if(final_label_id in ['hatespeech','offensive']):\n",
    "                    final_label_id='toxic'\n",
    "                else:\n",
    "                    final_label_id='non-toxic'\n",
    "                temp['final_label']=final_label_id\n",
    "\n",
    "        \n",
    "        else:\n",
    "            if(final_label.count(final_label_id)==1):\n",
    "                temp['final_label']='undecided'\n",
    "            else:\n",
    "                temp['final_label']=final_label_id\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        dict_data.append(temp)    \n",
    "    temp_read = pd.DataFrame(dict_data)  \n",
    "    return temp_read    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data_folder={\n",
    "      '2':{'data_file':'../../HateXplain/Data/dataset.json','class_label':'Data/classes_two.npy'},\n",
    "      '3':{'data_file':'../../HateXplain/Data/dataset.json','class_label':'Data/classes.npy'}\n",
    "}\n",
    "\n",
    "params = {}\n",
    "\n",
    "# We need to load the dataset with the labels as 'toxic' and 'non-toxic'. \n",
    "# We consider hatespeech and offensive as toxic and normal as non-toxic.\n",
    "params['num_classes']=2  \n",
    "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
    "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
    "\n",
    "data_all_labelled=get_annotated_data(params)\n",
    "data_all_labelled.head()\n",
    "data_all_labelled['processed'] = data_all_labelled['text'].apply(lambda x: ' '.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    11415\n",
       "0     7814\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['text'] = data_all_labelled['processed']\n",
    "df['label'] = data_all_labelled['final_label']\n",
    "df['label'].replace('toxic', 1, inplace=True)\n",
    "df['label'].replace('non-toxic', 0, inplace=True)\n",
    "\n",
    "df = df[df['label']!='undecided']\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; i am bit confused coz chinese ppl can not access twitter thn how this ching chong using it i think he pakistani ðŸ¤” ðŸ¤” ðŸ¤”</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this bitch in whataburger eating a burger with no top bun and holding it palm up i hate white bitches</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>laura loomer raped me while screaming at me in her disgusting kike language and said we must exterminate the goyim laura loomer loomergate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>and this is why i end up with nigger trainee doctors who can not speak properly lack basic knowledge of biology it truly scary if the public only knew</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nogs jews and dykes how enriching</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>guilty until proven innocent unless you are a jew a nigger or a lover of both</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tired i can not support abortion from a moral standpoint wired i can support it morally easy stop looking at those little niglets as future doctors and see them for what they really are future rapists of white women and future welfare recipient of white tax dollars and future democrat voters</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;number&gt; &lt;number&gt; &lt;percent&gt; of brits think multiculturalism is a failure i guess the other &lt;percent&gt; are happy with their muzzie master there no great in britain anymore soon big ben will be converted into a minaret</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>all of my exes were cute but they were hoes i guess i only attract fly looking thots ðŸ˜´</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;user&gt; condoning drug use not kike at all thanks for that disclosure</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                    text  \\\n",
       "3                                                                                                                                                                           <user> i am bit confused coz chinese ppl can not access twitter thn how this ching chong using it i think he pakistani ðŸ¤” ðŸ¤” ðŸ¤”   \n",
       "4                                                                                                                                                                                                  this bitch in whataburger eating a burger with no top bun and holding it palm up i hate white bitches   \n",
       "5                                                                                                                                                             laura loomer raped me while screaming at me in her disgusting kike language and said we must exterminate the goyim laura loomer loomergate   \n",
       "6                                                                                                                                                 and this is why i end up with nigger trainee doctors who can not speak properly lack basic knowledge of biology it truly scary if the public only knew   \n",
       "7                                                                                                                                                                                                                                                                      nogs jews and dykes how enriching   \n",
       "8                                                                                                                                                                                                                          guilty until proven innocent unless you are a jew a nigger or a lover of both   \n",
       "9   tired i can not support abortion from a moral standpoint wired i can support it morally easy stop looking at those little niglets as future doctors and see them for what they really are future rapists of white women and future welfare recipient of white tax dollars and future democrat voters   \n",
       "10                                                                               <number> <number> <percent> of brits think multiculturalism is a failure i guess the other <percent> are happy with their muzzie master there no great in britain anymore soon big ben will be converted into a minaret   \n",
       "14                                                                                                                                                                                                                all of my exes were cute but they were hoes i guess i only attract fly looking thots ðŸ˜´   \n",
       "19                                                                                                                                                                                                                                  <user> condoning drug use not kike at all thanks for that disclosure   \n",
       "\n",
       "   label  \n",
       "3      1  \n",
       "4      1  \n",
       "5      1  \n",
       "6      1  \n",
       "7      1  \n",
       "8      1  \n",
       "9      1  \n",
       "10     1  \n",
       "14     1  \n",
       "19     1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['label']==1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont think im getting my baby them white 9 he has two white j and nikes not even touched</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>we cannot continue calling ourselves feminists if the rights of all womxn arent addressed yes to a sexual offences public list but will a trans lesbian bisexual and queer womxn be able to enter their information on the reporting sheet gender forum</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nawt yall niggers ignoring me</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; i am bit confused coz chinese ppl can not access twitter thn how this ching chong using it i think he pakistani ðŸ¤” ðŸ¤” ðŸ¤”</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this bitch in whataburger eating a burger with no top bun and holding it palm up i hate white bitches</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>laura loomer raped me while screaming at me in her disgusting kike language and said we must exterminate the goyim laura loomer loomergate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>and this is why i end up with nigger trainee doctors who can not speak properly lack basic knowledge of biology it truly scary if the public only knew</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nogs jews and dykes how enriching</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>guilty until proven innocent unless you are a jew a nigger or a lover of both</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tired i can not support abortion from a moral standpoint wired i can support it morally easy stop looking at those little niglets as future doctors and see them for what they really are future rapists of white women and future welfare recipient of white tax dollars and future democrat voters</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                   text  \\\n",
       "0                                                                                                                                                                                                            i dont think im getting my baby them white 9 he has two white j and nikes not even touched   \n",
       "1                                               we cannot continue calling ourselves feminists if the rights of all womxn arent addressed yes to a sexual offences public list but will a trans lesbian bisexual and queer womxn be able to enter their information on the reporting sheet gender forum   \n",
       "2                                                                                                                                                                                                                                                                         nawt yall niggers ignoring me   \n",
       "3                                                                                                                                                                          <user> i am bit confused coz chinese ppl can not access twitter thn how this ching chong using it i think he pakistani ðŸ¤” ðŸ¤” ðŸ¤”   \n",
       "4                                                                                                                                                                                                 this bitch in whataburger eating a burger with no top bun and holding it palm up i hate white bitches   \n",
       "5                                                                                                                                                            laura loomer raped me while screaming at me in her disgusting kike language and said we must exterminate the goyim laura loomer loomergate   \n",
       "6                                                                                                                                                and this is why i end up with nigger trainee doctors who can not speak properly lack basic knowledge of biology it truly scary if the public only knew   \n",
       "7                                                                                                                                                                                                                                                                     nogs jews and dykes how enriching   \n",
       "8                                                                                                                                                                                                                         guilty until proven innocent unless you are a jew a nigger or a lover of both   \n",
       "9  tired i can not support abortion from a moral standpoint wired i can support it morally easy stop looking at those little niglets as future doctors and see them for what they really are future rapists of white women and future welfare recipient of white tax dollars and future democrat voters   \n",
       "\n",
       "  label  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     1  \n",
       "4     1  \n",
       "5     1  \n",
       "6     1  \n",
       "7     1  \n",
       "8     1  \n",
       "9     1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['text'].values\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word representation\n",
    "We are going to use TF*IDF (Term frequency- Inverse document frequency) vectorizer to convert our text into numbers.\n",
    "\n",
    "TF = Frequency of term\n",
    "\n",
    "IDF = No of docs/ No of docs with the term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19229, 5000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid = TfidfVectorizer(max_features=5000)\n",
    "x = tfid.fit_transform(x).toarray()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data\n",
    "  We split data into training, test and pool. Pool is the unlabelled pool we want to generate SHAP clusters for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "indices =  np.random.randint(low=0, high=x.shape[0], size=x.shape[0])\n",
    "train_indices = indices[0:round(0.7*x.shape[0])]\n",
    "test_indices = indices[round(0.7*x.shape[0]): round(0.9*x.shape[0])]\n",
    "pool_indices = indices[round(0.9*x.shape[0]):]\n",
    "df_train = df.iloc[train_indices]['text'].values\n",
    "df_test = df.iloc[test_indices]['text'].values\n",
    "df_pool = df.iloc[pool_indices]['text'].values\n",
    "x_train = x[train_indices]\n",
    "y_train = y[train_indices]\n",
    "x_test = x[test_indices]\n",
    "y_test = y[test_indices]\n",
    "x_pool = x[pool_indices]\n",
    "y_pool = y[pool_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model fitting\n",
    "We do a simple grid search to find the best SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7663030865630076\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_iter=1000\n",
    "C= [50]\n",
    "best_f1 = 0\n",
    "model = None\n",
    "for c in C:\n",
    "    m = SVC( max_iter=max_iter, C=c, kernel='rbf', class_weight='balanced', probability=True)\n",
    "    m.fit(x_train, y_train)\n",
    "    m.score(x_train,y_train)\n",
    "    predictions = m.predict(x_test)\n",
    "    f1 = f1_score(predictions, y_test)\n",
    "    if  f1 > best_f1:\n",
    "        model = m\n",
    "        best_f1 = f1\n",
    "        print(best_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.813075780089153"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure the f1_score and plot confusion matrix. Recall:\n",
    "- f1 score = 2PR/ (P+R)\n",
    "- Precision = actual positives/ predicted positives\n",
    "- Recall = predicted positives/ total actual positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.764249886000912, 0.7311492459698388)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(x_pool)\n",
    "f1_score(predictions, y_pool), accuracy_score(y_pool, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert predict probability to uncertainty. In binary classification this would be the same as 1-P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classwise_uncertainty = model.predict_proba(x_pool)\n",
    "uncertainty = 1 - np.max(classwise_uncertainty, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADjhJREFUeJzt3X+sZGV9x/H3R1GaKq3YvRC6rL1KtmmhsWBvKQlJg6GpCgmLKTSQVFdDs7bFVFP+cNUmmjak2x9qarS0ayQuiRWpP8K2UFukGOMfiBeyIuuWuuoW1t2wV2zQ1tRm4ds/7tkyJZc7596ZucM8+34lkznnmeec830ym8+efeacs6kqJEntet60C5AkTZZBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWrcKdMuAGDTpk01Pz8/7TIkaabcf//9362quWH9nhNBPz8/z+Li4rTLkKSZkuTf+/Rz6kaSGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhr3nLgzdhTzO++Y2rEP7bp8aseWpL48o5ekxhn0ktS4mZ+6kaRRtT4F7Bm9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWrc0KBPsiXJPUkOJNmf5G1d+3uTfCfJvu512cA270xyMMnDSV4zyQFIklbX53n0x4EbquqBJKcB9ye5q/vsA1X1F4Odk5wLXAOcB/w08PkkP1tVT46zcElSP0PP6KvqaFU90C3/ADgAbF5lk23ArVX1o6r6NnAQuHAcxUqS1m5Nc/RJ5oELgC93TW9N8mCSm5Oc3rVtBh4d2Owwq//FIEmaoN5Bn+TFwKeBt1fV94GbgHOA84GjwPtOdF1h81phfzuSLCZZXFpaWnPhkqR+egV9khewHPIfr6rPAFTVY1X1ZFU9BXyEp6dnDgNbBjY/GzjyzH1W1e6qWqiqhbm5uVHGIElaRZ+rbgJ8FDhQVe8faD9roNvrgYe65b3ANUlOTfJyYCtw3/hKliStRZ+rbi4G3gB8Lcm+ru1dwLVJzmd5WuYQ8BaAqtqf5Dbg6yxfsXO9V9xI0vQMDfqq+hIrz7vfuco2NwI3jlCXJGlMvDNWkhpn0EtS4wx6SWpcnx9jJZ1E5nfeMbVjH9p1+dSO3TLP6CWpcQa9JDXOoJekxhn0ktQ4f4yV9JwxzR+CW+YZvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktS4oUGfZEuSe5IcSLI/ydu69pcmuSvJN7r307v2JPlgkoNJHkzyqkkPQpL07Pqc0R8HbqiqnwcuAq5Pci6wE7i7qrYCd3frAK8DtnavHcBNY69aktTb0KCvqqNV9UC3/APgALAZ2Abs6brtAa7slrcBt9Sye4GXJDlr7JVLknpZ0xx9knngAuDLwJlVdRSW/zIAzui6bQYeHdjscNcmSZqC3kGf5MXAp4G3V9X3V+u6QlutsL8dSRaTLC4tLfUtQ5K0Rr2CPskLWA75j1fVZ7rmx05MyXTvx7r2w8CWgc3PBo48c59VtbuqFqpqYW5ubr31S5KG6HPVTYCPAgeq6v0DH+0FtnfL24HbB9rf2F19cxHwxIkpHknSxjulR5+LgTcAX0uyr2t7F7ALuC3JdcAjwNXdZ3cClwEHgR8Cbx5rxZKkNRka9FX1JVaedwe4dIX+BVw/Yl2SpDHxzlhJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWpcn0cgSJqC+Z13TLsENcIzeklqnGf0mgnTPLs9tOvyqR1bGgfP6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mN84YpaQgfRaBZ5xm9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNGxr0SW5OcizJQwNt703ynST7utdlA5+9M8nBJA8nec2kCpck9dPnjP5jwGtXaP9AVZ3fve4ESHIucA1wXrfNXyV5/riKlSSt3dCgr6ovAt/rub9twK1V9aOq+jZwELhwhPokSSMaZY7+rUke7KZ2Tu/aNgOPDvQ53LVJkqZkvUF/E3AOcD5wFHhf154V+tZKO0iyI8liksWlpaV1liFJGmZdQV9Vj1XVk1X1FPARnp6eOQxsGeh6NnDkWfaxu6oWqmphbm5uPWVIknpYV9AnOWtg9fXAiSty9gLXJDk1ycuBrcB9o5UoSRrF0McUJ/kEcAmwKclh4D3AJUnOZ3la5hDwFoCq2p/kNuDrwHHg+qp6cjKlS5L6GBr0VXXtCs0fXaX/jcCNoxQlSRof74yVpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjRt6eaWee+Z33jG1Yx/adfnUji1pfTyjl6TGeUavNZnmvyYkrY9n9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjhgZ9kpuTHEvy0EDbS5PcleQb3fvpXXuSfDDJwSQPJnnVJIuXJA3X5z8H/xjwIeCWgbadwN1VtSvJzm79HcDrgK3d61eAm7r3JvkfZUuaBUPP6Kvqi8D3ntG8DdjTLe8Brhxov6WW3Qu8JMlZ4ypWkrR2652jP7OqjgJ072d07ZuBRwf6He7aJElTMu4fY7NCW63YMdmRZDHJ4tLS0pjLkCSdsN6gf+zElEz3fqxrPwxsGeh3NnBkpR1U1e6qWqiqhbm5uXWWIUkaZr1BvxfY3i1vB24faH9jd/XNRcATJ6Z4JEnTMfSqmySfAC4BNiU5DLwH2AXcluQ64BHg6q77ncBlwEHgh8CbJ1CzJGkNhgZ9VV37LB9dukLfAq4ftShJ0vh4Z6wkNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIad8ooGyc5BPwAeBI4XlULSV4KfBKYBw4Bv1lV/zFamZKk9RrHGf2rq+r8qlro1ncCd1fVVuDubl2SNCWTmLrZBuzplvcAV07gGJKknkYN+gL+Ocn9SXZ0bWdW1VGA7v2MEY8hSRrBSHP0wMVVdSTJGcBdSf6174bdXww7AF72speNWIYk6dmMdEZfVUe692PAZ4ELgceSnAXQvR97lm13V9VCVS3Mzc2NUoYkaRXrDvokL0py2oll4NeBh4C9wPau23bg9lGLlCSt3yhTN2cCn01yYj9/W1WfS/IV4LYk1wGPAFePXqYkab3WHfRV9S3gF1dofxy4dJSiJEnj452xktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJatzEgj7Ja5M8nORgkp2TOo4kaXUTCfokzwc+DLwOOBe4Nsm5kziWJGl1kzqjvxA4WFXfqqr/AW4Ftk3oWJKkVUwq6DcDjw6sH+7aJEkb7JQJ7TcrtNX/65DsAHZ0q/+Z5OF1HmsT8N11bjurHPPJwTGfBPKnI435Z/p0mlTQHwa2DKyfDRwZ7FBVu4Hdox4oyWJVLYy6n1nimE8OjvnksBFjntTUzVeArUlenuSFwDXA3gkdS5K0iomc0VfV8SRvBf4JeD5wc1Xtn8SxJEmrm9TUDVV1J3DnpPY/YOTpnxnkmE8OjvnkMPExp6qG95IkzSwfgSBJjZuZoB/2SIUkpyb5ZPf5l5PMb3yV49VjzL+a5IEkx5NcNY0ax63HmP8gydeTPJjk7iS9Li97Lusx5t9J8rUk+5J8qYW7zPs+IiXJVUkqyUxfidPjO35TkqXuO96X5LfHWkBVPedfLP+g+03gFcALga8C5z6jz+8Bf90tXwN8ctp1b8CY54FXArcAV0275g0a86uBH++Wf/ck+Z5/YmD5CuBz06570mPu+p0GfBG4F1iYdt0T/o7fBHxoUjXMyhl9n0cqbAP2dMufAi5NstKNW7Ni6Jir6lBVPQg8NY0CJ6DPmO+pqh92q/eyfI/GLOsz5u8PrL6IZ9x8OIP6PiLlj4E/A/57I4ubgKk/EmZWgr7PIxX+r09VHQeeAH5qQ6qbjJPxMRJrHfN1wD9OtKLJ6zXmJNcn+SbLwff7G1TbpAwdc5ILgC1V9Q8bWdiE9P1z/RvdlOSnkmxZ4fN1m5WgH/pIhZ59Zklr4+mj95iT/BawAPz5RCuavF5jrqoPV9U5wDuAP5x4VZO16piTPA/4AHDDhlU0WX2+478H5qvqlcDneXp2YixmJeiHPlJhsE+SU4CfBL63IdVNRp8xt6bXmJP8GvBu4Iqq+tEG1TYpa/2ebwWunGhFkzdszKcBvwB8Ickh4CJg7wz/INvnkTCPD/xZ/gjwS+MsYFaCvs8jFfYC27vlq4B/qe5Xjhl1Mj5GYuiYu3/S/w3LIX9sCjWOW58xbx1YvRz4xgbWNwmrjrmqnqiqTVU1X1XzLP8Wc0VVLU6n3JH1+Y7PGli9Ajgw1gqm/Yv0Gn65vgz4N5Z/vX531/ZHLP8BAPgx4O+Ag8B9wCumXfMGjPmXWT5b+C/gcWD/tGvegDF/HngM2Ne99k675g0Y818C+7vx3gOcN+2aJz3mZ/T9AjN81U3P7/hPuu/4q913/HPjPL53xkpS42Zl6kaStE4GvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjftfkpMXHts5pyAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(uncertainty)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2015095a1d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix(y_pool, predictions),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain model using SHAP\n",
    "\n",
    "Refer https://christophm.github.io/interpretable-ml-book/shap.html\n",
    "\n",
    "The goal of SHAP is to explain the prediction of an instance x by computing the contribution of each feature to the prediction. The SHAP explanation method computes Shapley values from coalitional game theory. The feature values of a data instance act as players in a coalition. Shapley values tell us how to fairly distribute the â€œpayoutâ€ (= the prediction) among the features\n",
    "\n",
    "We call the SHAP explainer for linear models\n",
    "shapely values produced have same dimensions as data passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af51cde1ac4345d2a8adc4ce76e76e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13460.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explainer = shap.KernelExplainer(model.predict_proba, shap.kmeans(x_train, 100))\n",
    "shap_values_train = explainer.shap_values(x_train)\n",
    "shap_values_pool = explainer.shap_values(x_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_pool.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain a single positive prediction at 'index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postive_index = 0\n",
    "index = np.where(predictions==1)[0][postive_index]\n",
    "print(\"text \", df_test[index], \" prediction: \", predictions[index], \"actual \", y_test[index])\n",
    "shap.force_plot(explainer.expected_value, \n",
    "                               shap_values_pool[index,:], \n",
    "                               x_test[index,:], feature_names = tfid.get_feature_names(),\n",
    "               matplotlib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the overall feature importance using summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_pool, x_pool, feature_names=tfid.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_pool.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering data\n",
    "We are going to cluster the training data using SHAP explanations (shapely space)\n",
    "SHAP clustering works by clustering on Shapley values of each instance. \n",
    "This means that you cluster instances by explanation similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 1\n",
    "kmeans = KMeans(n_clusters= n_clusters, n_jobs=-1, max_iter=600)\n",
    "kmeans.fit(shap_values_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homogeneity_score( y_pool, kmeans.labels_), v_measure_score(y_pool, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use cosine distance instead of euclidean distance to measure the similarity between the documents.\n",
    "As the size of the document increases, the number of common words (euclidean) tend to increase \n",
    "even if the documents talk about different topics. The cosine similarity helps overcome this fundamental flaw \n",
    "and finds the similarity irrespective of size.\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/cosine-similarity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similarity of each point in cluster to its centroid\n",
    "similarity_to_center = []\n",
    "for i, instance in enumerate(shap_values_pool):\n",
    "    cluster_label = kmeans.labels_[i] # cluster of this instance\n",
    "    centroid = kmeans.cluster_centers_[cluster_label] # cluster center of the cluster of that instance\n",
    "    similarity = 1-cosine(instance, centroid) # 1- cosine distance gives similarity\n",
    "    similarity_to_center.append(similarity)\n",
    "    \n",
    "centroid_match = [None]*n_clusters\n",
    "centroid_indices =[None]*n_clusters\n",
    "for i, instance in enumerate(shap_values_pool):\n",
    "    cluster_label = kmeans.labels_[i]     \n",
    "    if centroid_match[cluster_label] is None or similarity_to_center[i] > centroid_match[cluster_label]:\n",
    "        centroid_indices[cluster_label] = i\n",
    "        centroid_match[cluster_label] = similarity_to_center[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, perplexity=20)\n",
    "principals = tsne.fit_transform(shap_values_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "colorscale = [[0, 'mediumturquoise'], [1, 'salmon']]\n",
    "collect = dict()\n",
    "sizes = []\n",
    "color = ['hsl(' + str(h) + ',80%' + ',50%)' for h in np.linspace(0, 255, n_clusters)]\n",
    "df_final_labels = pd.DataFrame()\n",
    "for cluster_id in np.unique(kmeans.labels_):\n",
    "    cluster_indices = np.where(kmeans.labels_ == cluster_id)    \n",
    "    \n",
    "    cluster_text = df_pool[cluster_indices]\n",
    "    center_index = centroid_indices[cluster_id]\n",
    "    center_text = df_pool[center_index]\n",
    "    sizes.append(len(cluster_indices[0]))\n",
    "    df_cluster = pd.DataFrame({'text': cluster_text})\n",
    "    df_cluster['cluster_id'] = cluster_id\n",
    "    df_cluster['centroid'] = False\n",
    "    df_cluster = df_cluster.append({'text':center_text, 'cluster_id':cluster_id,\n",
    "                                    'centroid':True }, ignore_index=True)\n",
    "    df_final_labels = pd.concat([df_final_labels, df_cluster])\n",
    "\n",
    "    cp = principals[cluster_indices]\n",
    "    data.append(go.Heatmap(x=cp[:, 0],\n",
    "                           y=cp[:, 1],\n",
    "                           z=uncertainty[cluster_indices],\n",
    "                           name='uncertainity map',\n",
    "                           visible=True,\n",
    "                           showscale=False,\n",
    "                           colorscale=colorscale,\n",
    "                                         ))\n",
    "    data.append(go.Scatter(x = cp[:,0],\n",
    "                   y = cp[:,1],\n",
    "                   mode='markers',                    \n",
    "                hovertext=cluster_text,\n",
    "                            marker=dict(color=color[cluster_id],\n",
    "                                                   size=10),\n",
    "                           name = 'cluster '+ str(cluster_id)\n",
    "                          ))\n",
    "    data.append(go.Scatter(x = [principals[center_index, 0]],\n",
    "                   y = [principals[center_index, 1]],\n",
    "                   mode='markers',  \n",
    "                           marker=dict(color=color[cluster_id],\n",
    "                                                   size=15,\n",
    "                                                   line=dict(color='black', width=5)),\n",
    "                           name = 'centroid cluster '+ str(cluster_id),\n",
    "                           visible='legendonly',\n",
    "                           \n",
    "                          ))\n",
    "    collect[cluster_id] = df_pool[cluster_indices]\n",
    "    \n",
    "fig = go.Figure(data=data)\n",
    "fig.show()\n",
    "df_final_labels.to_csv('df_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze uncertainty within clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "ranges = []\n",
    "for cluster_id in np.unique(kmeans.labels_):\n",
    "    cluster_indices = np.where(kmeans.labels_ == cluster_id)    \n",
    "    uncertainty_cluster = uncertainty[cluster_indices]\n",
    "    rng = np.max(uncertainty_cluster)- np.min(uncertainty_cluster)\n",
    "    print(cluster_id, \"range \", rng)\n",
    "    ranges.append(rng)\n",
    "    print(y_pool[cluster_indices])\n",
    "    print(\"\\n\")\n",
    "    data.append(go.Histogram(x=uncertainty_cluster, name=str(cluster_id), showlegend=True,visible='legendonly'))\n",
    "fig = go.Figure(data=data)\n",
    "url=py.plot(fig, filename='clusters_50', sharing='public')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_uncertain_indices = (-uncertainty).argsort()[:20]\n",
    "y_pool[max_uncertain_indices], uncertainty[max_uncertain_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_id = 8\n",
    "cluster_indices = np.where(kmeans.labels_ == cluster_id)    \n",
    "d = {'text' : df_pool[cluster_indices], 'uncertainty': uncertainty[cluster_indices], 'label': y_pool[cluster_indices]}\n",
    "\n",
    "pd.DataFrame(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=cluster_indices[0][10]\n",
    "shap.force_plot(explainer.expected_value, shap_values_pool[i,:], x_pool[i,:], feature_names = tfid.get_feature_names(),\n",
    "               matplotlib=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find optimal cluster size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homogeneity_scores = []\n",
    "v_measure_scores = []\n",
    "completeness_scores = []\n",
    "n_iters = 10\n",
    "ranges = list(range(10, 110, 10))\n",
    "for k in ranges:\n",
    "    vavg = 0\n",
    "    havg = 0\n",
    "    cavg = 0\n",
    "    for i in range(n_iters):\n",
    "        kmeans = KMeans(n_clusters= n_clusters, n_jobs=-1)\n",
    "        kmeans.fit(shap_values_pool)\n",
    "        v = v_measure_score(labels_pred=kmeans.labels_, labels_true=y_pool) \n",
    "        h = homogeneity_score(labels_pred=kmeans.labels_, labels_true=y_pool) \n",
    "        c = completeness_score(labels_pred=kmeans.labels_, labels_true=y_pool) \n",
    "        vavg += v\n",
    "        havg += h\n",
    "        cavg += c\n",
    "    homogeneity_scores.append(havg/n_iters)\n",
    "    v_measure_scores.append(vavg/n_iters)\n",
    "    completeness_scores.append(cavg/n_iters)\n",
    "    print(k, \"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [go.Scatter(x=ranges, y=homogeneity_scores, mode=\"lines\", name=\"homogeneity\"),\n",
    "        go.Scatter(x=ranges, y=v_measure_scores, mode=\"lines\", name=\"v_measure\"),\n",
    "        go.Scatter(x=ranges, y=completeness_scores, mode=\"lines\", name=\"completeness\")\n",
    "        ]\n",
    "fig = go.Figure(data=data)\n",
    "fig.update_layout(xaxis_title=\"no of clusters\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add centroid of each cluster to the training set and retrain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices_new = np.append(train_indices, centroid_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = SVC( max_iter=max_iter, C=C, kernel='linear')\n",
    "x_train_new = x[train_indices_new]\n",
    "y_train_new = y[train_indices_new]\n",
    "model1.fit(x_train_new, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.score(x_train_new, y_train_new), model1.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = model1.predict(x_test)\n",
    "f1_score(y_test, predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predictions1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add entire x_pool back to training instead of just centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices_full = np.append(train_indices, pool_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = SVC(max_iter=max_iter, C=C, kernel='linear')\n",
    "x_train_full = x[train_indices_full]\n",
    "y_train_full = y[train_indices_full]\n",
    "model2.fit(x_train_full, y_train_full)\n",
    "model2.score(x_train_full, y_train_full), model2.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = model2.predict(x_test)\n",
    "f1_score(y_test, predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model with 20% train \", f1_score(y_test, predictions))\n",
    "print(\"Model with 20% train + center \", f1_score(y_test, predictions1))\n",
    "print(\"Model with 20% train + 60% pool \", f1_score(y_test, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model with 20% train \", accuracy_score(y_test, predictions))\n",
    "print(\"Model with 20% train + center \", accuracy_score(y_test, predictions1))\n",
    "print(\"Model with 20% train + 60% pool \", accuracy_score(y_test, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

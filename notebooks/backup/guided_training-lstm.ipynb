{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Hate_xplain_shap.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H03PlCG_r3Rb"
      },
      "source": [
        "## 1. Install dependencies and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J5P4ugEmiS8G",
        "outputId": "e42ab5c2-a0f4-4db4-a84d-bd1ea21fd32a"
      },
      "source": [
        "!pip install -U tensorflow-gpu==1.15.0 --force-reinstall"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 43kB/s \n",
            "\u001b[?25hCollecting absl-py>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/58/0aa6fb779dc69cfc811df3398fcbeaeefbf18561b6e36b185df0782781cc/absl_py-0.11.0-py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 38.7MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/fd/247ef25f5ec5f9acecfbc98ca3c6aaf66716cf52509aca9a93583d410493/protobuf-3.14.0-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 35.5MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting wrapt>=1.11.1\n",
            "  Downloading https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz\n",
            "Collecting grpcio>=1.8.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/55/357022b111b856cadaf63d718d79861fc6215b848eff38b2fbfb9d5c47bd/grpcio-1.35.0-cp36-cp36m-manylinux2014_x86_64.whl (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 31.5MB/s \n",
            "\u001b[?25hCollecting wheel>=0.26\n",
            "  Downloading https://files.pythonhosted.org/packages/65/63/39d04c74222770ed1589c0eaba06c05891801219272420b40311cd60c880/wheel-0.36.2-py2.py3-none-any.whl\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 37.6MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.8MB/s \n",
            "\u001b[?25hCollecting astor>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
            "Collecting opt-einsum>=2.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.5MB/s \n",
            "\u001b[?25hCollecting termcolor>=1.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 31.5MB/s \n",
            "\u001b[?25hCollecting numpy<2.0,>=1.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/32/d3fa649ad7ec0b82737b92fefd3c4dd376b0bb23730715124569f38f3a08/numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8MB 186kB/s \n",
            "\u001b[?25hCollecting keras-preprocessing>=1.0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.7MB/s \n",
            "\u001b[?25hCollecting six>=1.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
            "Collecting h5py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/7a/e53e500335afb6b1aade11227cdf107fca54106a1dca5c9d13242a043f3b/h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 37.1MB/s \n",
            "\u001b[?25hCollecting setuptools>=41.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/0e/255e3d57965f318973e417d5b7034223f1223de500d91b945ddfaef42a37/setuptools-53.0.0-py3-none-any.whl (784kB)\n",
            "\u001b[K     |████████████████████████████████| 788kB 32.5MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/ef/24a91ca96efa0d7802dffb83ccc7a3c677027bea19ec3c9ee80be740408e/Markdown-3.3.3-py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.7MB/s \n",
            "\u001b[?25hCollecting werkzeug>=0.11.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 44.6MB/s \n",
            "\u001b[?25hCollecting cached-property; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
            "Collecting importlib-metadata; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/f3/ed/da40116a204abb5c4dd1d929346d33e0d29cedb2cedd18ea98f0385dcd92/importlib_metadata-3.4.0-py3-none-any.whl\n",
            "Collecting zipp>=0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/41/ad/6a4f1a124b325618a7fb758b885b68ff7b058eec47d9220a12ab38d90b1f/zipp-3.4.0-py3-none-any.whl\n",
            "Collecting typing-extensions>=3.6.4; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
            "Building wheels for collected packages: gast, wrapt, termcolor\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=aa560449e89e1c2c5a555612c0b63c8c2da9673b73b7fb326fb15c2ea4a15fed\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=67492 sha256=dabdb885e4f90b560be19d4a0cfa6500467a5fc8d2ce3e837b1ae4d03949fdd3\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/c2/ed/d62208260edbd3fa7156545c00ef966f45f2063d0a84f8208a\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-cp36-none-any.whl size=4830 sha256=4b1696429d08812f9f9be9d7a40670daf733786bc8c07d06378d24fdc0826277\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
            "Successfully built gast wrapt termcolor\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement grpcio~=1.32.0, but you'll have grpcio 1.35.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement h5py~=2.10.0, but you'll have h5py 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorboard~=2.4, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorflow-estimator<2.5.0,>=2.4.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-metadata 0.27.0 has requirement absl-py<0.11,>=0.9, but you'll have absl-py 0.11.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: nbclient 0.5.1 has requirement jupyter-client>=6.1.5, but you'll have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: six, absl-py, protobuf, gast, wrapt, grpcio, wheel, tensorflow-estimator, numpy, cached-property, h5py, keras-applications, astor, opt-einsum, termcolor, setuptools, zipp, typing-extensions, importlib-metadata, markdown, werkzeug, tensorboard, keras-preprocessing, google-pasta, tensorflow-gpu\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Found existing installation: absl-py 0.10.0\n",
            "    Uninstalling absl-py-0.10.0:\n",
            "      Successfully uninstalled absl-py-0.10.0\n",
            "  Found existing installation: protobuf 3.12.4\n",
            "    Uninstalling protobuf-3.12.4:\n",
            "      Successfully uninstalled protobuf-3.12.4\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Found existing installation: grpcio 1.32.0\n",
            "    Uninstalling grpcio-1.32.0:\n",
            "      Successfully uninstalled grpcio-1.32.0\n",
            "  Found existing installation: wheel 0.36.2\n",
            "    Uninstalling wheel-0.36.2:\n",
            "      Successfully uninstalled wheel-0.36.2\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "  Found existing installation: astor 0.8.1\n",
            "    Uninstalling astor-0.8.1:\n",
            "      Successfully uninstalled astor-0.8.1\n",
            "  Found existing installation: opt-einsum 3.3.0\n",
            "    Uninstalling opt-einsum-3.3.0:\n",
            "      Successfully uninstalled opt-einsum-3.3.0\n",
            "  Found existing installation: termcolor 1.1.0\n",
            "    Uninstalling termcolor-1.1.0:\n",
            "      Successfully uninstalled termcolor-1.1.0\n",
            "  Found existing installation: setuptools 53.0.0\n",
            "    Uninstalling setuptools-53.0.0:\n",
            "      Successfully uninstalled setuptools-53.0.0\n",
            "  Found existing installation: zipp 3.4.0\n",
            "    Uninstalling zipp-3.4.0:\n",
            "      Successfully uninstalled zipp-3.4.0\n",
            "  Found existing installation: typing-extensions 3.7.4.3\n",
            "    Uninstalling typing-extensions-3.7.4.3:\n",
            "      Successfully uninstalled typing-extensions-3.7.4.3\n",
            "  Found existing installation: importlib-metadata 3.4.0\n",
            "    Uninstalling importlib-metadata-3.4.0:\n",
            "      Successfully uninstalled importlib-metadata-3.4.0\n",
            "  Found existing installation: Markdown 3.3.3\n",
            "    Uninstalling Markdown-3.3.3:\n",
            "      Successfully uninstalled Markdown-3.3.3\n",
            "  Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "Successfully installed absl-py-0.11.0 astor-0.8.1 cached-property-1.5.2 gast-0.2.2 google-pasta-0.2.0 grpcio-1.35.0 h5py-3.1.0 importlib-metadata-3.4.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.3 numpy-1.19.5 opt-einsum-3.3.0 protobuf-3.14.0 setuptools-53.0.0 six-1.15.0 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0 termcolor-1.1.0 typing-extensions-3.7.4.3 werkzeug-1.0.1 wheel-0.36.2 wrapt-1.12.1 zipp-3.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "astor",
                  "google",
                  "numpy",
                  "pkg_resources",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzmUvLtWq7H7",
        "outputId": "c459078d-bea0-46a2-cf5f-b8b370207e45"
      },
      "source": [
        "!pip install shap"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting shap\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/20/54381999efe3000f70a7f68af79ba857cfa3f82278ab0e02e6ba1c06b002/shap-0.38.1.tar.gz (352kB)\n",
            "\r\u001b[K     |█                               | 10kB 18.7MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 21.3MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30kB 11.5MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40kB 9.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 61kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 71kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 81kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 92kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 102kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 112kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 122kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 143kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 153kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 163kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 174kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 184kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 194kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 204kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 215kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 225kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 235kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 245kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 256kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 266kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 276kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 286kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 296kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 307kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 317kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 327kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 337kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 348kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 358kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from shap) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from shap) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from shap) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from shap) (1.1.5)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.6/dist-packages (from shap) (4.41.1)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/c2/b3f55dfdb8af9812fdb9baf70cacf3b9e82e505b2bd4324d588888b81202/slicer-0.0.7-py3-none-any.whl\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from shap) (0.51.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from shap) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->shap) (1.0.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->shap) (53.0.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.6/dist-packages (from numba->shap) (0.34.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
            "Building wheels for collected packages: shap\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.38.1-cp36-cp36m-linux_x86_64.whl size=489391 sha256=0cba3ab2ce100e899fec5235c0ae4e275531ae2367d20e0e616f65d541dd4931\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/fb/e4/88012be41842b9be62ae18d82d1b1e880daf8539d1fef1fa00\n",
            "Successfully built shap\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.38.1 slicer-0.0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQvp__3urMjr",
        "outputId": "7b3796cc-ab36-4af9-bbcf-622515c0f744"
      },
      "source": [
        "!git clone https://github.com/punyajoy/HateXplain.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'HateXplain'...\n",
            "remote: Enumerating objects: 117, done.\u001b[K\n",
            "remote: Counting objects: 100% (117/117), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 367 (delta 60), reused 50 (delta 21), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (367/367), 4.80 MiB | 14.14 MiB/s, done.\n",
            "Resolving deltas: 100% (201/201), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFyqprpNravi"
      },
      "source": [
        "import json\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from scipy.spatial.distance import euclidean, cosine\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, homogeneity_score, v_measure_score, completeness_score\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import plotly\r\n",
        "import plotly.graph_objs as go\r\n",
        "# import chart_studio.plotly as py\r\n",
        "import seaborn as sns\r\n",
        "import shap\r\n",
        "\r\n",
        "\r\n",
        "from tensorflow.keras.preprocessing import sequence\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Embedding\r\n",
        "from tensorflow.keras.layers import LSTM\r\n",
        "from tensorflow.keras.datasets import imdb\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, homogeneity_score\r\n",
        "\r\n",
        "pd.set_option('display.max_colwidth', 1000)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqlY7kSZsR_J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b9723a-77c7-45a9-c76e-02760290b38f"
      },
      "source": [
        "from tensorflow.python.client import device_lib\r\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 12718192240676944155\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 2994474280056653599\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 15391528335996094549\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15956099072\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 7257519992063862477\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsnVbnVSrfeJ"
      },
      "source": [
        "def get_annotated_data(params):\r\n",
        "    #temp_read = pd.read_pickle(params['data_file'])\r\n",
        "    with open(params['data_file'], 'r') as fp:\r\n",
        "        data = json.load(fp)\r\n",
        "    dict_data=[]\r\n",
        "    for key in data:\r\n",
        "        temp={}\r\n",
        "        temp['post_id']=key\r\n",
        "        temp['text']=data[key]['post_tokens']\r\n",
        "        final_label=[]\r\n",
        "        for i in range(1,4):\r\n",
        "            temp['annotatorid'+str(i)]=data[key]['annotators'][i-1]['annotator_id']\r\n",
        "#             temp['explain'+str(i)]=data[key]['annotators'][i-1]['rationales']\r\n",
        "            temp['target'+str(i)]=data[key]['annotators'][i-1]['target']\r\n",
        "            temp['label'+str(i)]=data[key]['annotators'][i-1]['label']\r\n",
        "            final_label.append(temp['label'+str(i)])\r\n",
        "\r\n",
        "        final_label_id=max(final_label,key=final_label.count)\r\n",
        "        temp['rationales']=data[key]['rationales']\r\n",
        "            \r\n",
        "        if(params['class_names']=='Data/classes_two.npy'):\r\n",
        "            if(final_label.count(final_label_id)==1):\r\n",
        "                temp['final_label']='undecided'\r\n",
        "            else:\r\n",
        "                if(final_label_id in ['hatespeech','offensive']):\r\n",
        "                    final_label_id='toxic'\r\n",
        "                else:\r\n",
        "                    final_label_id='non-toxic'\r\n",
        "                temp['final_label']=final_label_id\r\n",
        "\r\n",
        "        \r\n",
        "        else:\r\n",
        "            if(final_label.count(final_label_id)==1):\r\n",
        "                temp['final_label']='undecided'\r\n",
        "            else:\r\n",
        "                temp['final_label']=final_label_id\r\n",
        "\r\n",
        "        \r\n",
        "        \r\n",
        "        \r\n",
        "        dict_data.append(temp)    \r\n",
        "    temp_read = pd.DataFrame(dict_data)  \r\n",
        "    return temp_read    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvHDTx82sHvq"
      },
      "source": [
        "## 2.Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3jcMyakrp_J"
      },
      "source": [
        "dict_data_folder={\r\n",
        "      '2':{'data_file':'HateXplain/Data/dataset.json','class_label':'Data/classes_two.npy'},\r\n",
        "      '3':{'data_file':'HateXplain/Data/dataset.json','class_label':'Data/classes.npy'}\r\n",
        "}\r\n",
        "\r\n",
        "params = {}\r\n",
        "\r\n",
        "# We need to load the dataset with the labels as 'toxic' and 'non-toxic'. \r\n",
        "# We consider hatespeech and offensive as toxic and normal as non-toxic.\r\n",
        "params['num_classes']=2  \r\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\r\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\r\n",
        "\r\n",
        "data_all_labelled=get_annotated_data(params)\r\n",
        "data_all_labelled.head()\r\n",
        "data_all_labelled['processed'] = data_all_labelled['text'].apply(lambda x: ' '.join(map(str, x)))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tTcVlV1rwoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "448a8039-9b0b-4add-97f7-3727517c6544"
      },
      "source": [
        "df = pd.DataFrame()\r\n",
        "df['text'] = data_all_labelled['processed']\r\n",
        "df['label'] = data_all_labelled['final_label']\r\n",
        "df['label'].replace('toxic', 1, inplace=True)\r\n",
        "df['label'].replace('non-toxic', 0, inplace=True)\r\n",
        "\r\n",
        "df = df[df['label']!='undecided']\r\n",
        "df['label'].value_counts()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    11415\n",
              "0     7814\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzw2xqxJs_lz"
      },
      "source": [
        "## 3. Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OESMUtItWIp"
      },
      "source": [
        "x = df['text'].values\r\n",
        "y = df['label'].values"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzq02FrLtb8b"
      },
      "source": [
        "indices =  np.random.randint(low=0, high=x.shape[0], size=x.shape[0])\r\n",
        "train_indices = indices[0:round(0.8*x.shape[0])]\r\n",
        "pool_indices = indices[round(0.8*x.shape[0]):]\r\n",
        "df_train = df.iloc[train_indices]['text'].values\r\n",
        "df_test = df.iloc[pool_indices]['text'].values\r\n",
        "y_train = y[train_indices]\r\n",
        "y_test = y[pool_indices]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIHdozZSuVia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d157d5-31f7-4f72-be31-c45f0f61667d"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\r\n",
        "tokenizer.fit_on_texts(df_train)\r\n",
        "\r\n",
        "X_train = tokenizer.texts_to_sequences(df_train)\r\n",
        "X_test = tokenizer.texts_to_sequences(df_test)\r\n",
        "\r\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\r\n",
        "print(df_train[2])\r\n",
        "print(X_train[2])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dear german cousins i hear you are under attack from false guilt muslim scum heavy censorship i cannot speak for you but from australia fuck merkel\n",
            "[822, 1081, 3306, 5, 646, 7, 12, 469, 428, 50, 1266, 1354, 61, 489, 2249, 5, 615, 616, 17, 7, 28, 50, 1442, 75, 2250]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrA8ODLAr0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba78a77c-3810-4e2a-c710-2ba495826657"
      },
      "source": [
        "max_features = vocab_size\r\n",
        "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\r\n",
        "batch_size = 16\r\n",
        "n_epochs = 10\r\n",
        "print('Loading data...')\r\n",
        "(x_train, y_train), (x_test, y_test) = (X_train, y_train), (X_test, y_test)\r\n",
        "print(len(x_train), 'train sequences')\r\n",
        "print(len(x_test), 'test sequences')\r\n",
        "\r\n",
        "print('Pad sequences (samples x time)')\r\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\r\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\r\n",
        "print('x_train shape:', x_train.shape)\r\n",
        "print('x_test shape:', x_test.shape)\r\n",
        "x_train = np.asarray(x_train).astype(np.float32)\r\n",
        "y_train =np.asarray(y_train).astype(np.float32)\r\n",
        "x_test =np.asarray(x_test).astype(np.float32)\r\n",
        "y_test =np.asarray(y_test).astype(np.float32)\r\n",
        "print('Build model...')\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(vocab_size, 128))\r\n",
        "model.add(LSTM(128, dropout=0.4, recurrent_dropout=0.2))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "# try using different optimizers and different optimizer configs\r\n",
        "model.compile(loss='binary_crossentropy',\r\n",
        "              optimizer='adam',\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "print('Train...')\r\n",
        "history  = model.fit(x_train, y_train,\r\n",
        "          batch_size=batch_size,\r\n",
        "          epochs=n_epochs,\r\n",
        "          validation_data=(x_test, y_test))\r\n",
        "score, acc = model.evaluate(x_test, y_test,\r\n",
        "                            batch_size=batch_size)\r\n",
        "print('Test score:', score)\r\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "15383 train sequences\n",
            "3846 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (15383, 80)\n",
            "x_test shape: (3846, 80)\n",
            "Build model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train...\n",
            "Train on 15383 samples, validate on 3846 samples\n",
            "Epoch 1/10\n",
            "15383/15383 [==============================] - 163s 11ms/sample - loss: 0.5201 - acc: 0.7328 - val_loss: 0.4515 - val_acc: 0.7871\n",
            "Epoch 2/10\n",
            "15383/15383 [==============================] - 162s 11ms/sample - loss: 0.3597 - acc: 0.8411 - val_loss: 0.4019 - val_acc: 0.8219\n",
            "Epoch 3/10\n",
            "15383/15383 [==============================] - 162s 11ms/sample - loss: 0.2769 - acc: 0.8845 - val_loss: 0.4101 - val_acc: 0.8287\n",
            "Epoch 4/10\n",
            "15383/15383 [==============================] - 163s 11ms/sample - loss: 0.2114 - acc: 0.9151 - val_loss: 0.4976 - val_acc: 0.8266\n",
            "Epoch 5/10\n",
            "15383/15383 [==============================] - 162s 11ms/sample - loss: 0.1708 - acc: 0.9323 - val_loss: 0.4699 - val_acc: 0.8463\n",
            "Epoch 6/10\n",
            "15383/15383 [==============================] - 162s 11ms/sample - loss: 0.1397 - acc: 0.9474 - val_loss: 0.4978 - val_acc: 0.8484\n",
            "Epoch 7/10\n",
            " 8656/15383 [===============>..............] - ETA: 1:07 - loss: 0.1026 - acc: 0.9614"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBlQIIMg0Ofp"
      },
      "source": [
        "# Plot epochs vs train and test scores\r\n",
        "# data = [go.Scatter(x=list(range(n_epochs)), y=homogeneity_scores, mode=\"lines\", name=\"homogeneity\"),\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X23BkFG6hQ_h"
      },
      "source": [
        "predictions = model.predict_classes(x_test, verbose=1)\r\n",
        "print('Overall F1 Score', f1_score(predictions, y_test))\r\n",
        "sns.heatmap(confusion_matrix(y_test, predictions),annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHgXdI6thnrx"
      },
      "source": [
        "np.unique(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJzb5Dcau3uS"
      },
      "source": [
        "import shap\r\n",
        "\r\n",
        "# we use the first 100 training examples as our background dataset to integrate over\r\n",
        "explainer = shap.DeepExplainer(model, x_train[:5000])\r\n",
        "\r\n",
        "# explain the first 10 predictions\r\n",
        "# explaining each prediction requires 2 * background dataset size runs\r\n",
        "shap_values = explainer.shap_values(x_test[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMDQvNvujQoC"
      },
      "source": [
        "shap_values[0][0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhUfBh-eu8EZ"
      },
      "source": [
        "# Creating a reverse dictionary\r\n",
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\r\n",
        "shap.summary_plot(shap_values[0], x_test[:100], reverse_word_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFsZeKfhV9vo"
      },
      "source": [
        "# transform the indexes to words\r\n",
        "import numpy as np\r\n",
        "# use number to words map to get x_test in form of words\r\n",
        "num2word = reverse_word_map\r\n",
        "x_test_words = np.stack([np.array(list(map(lambda x: num2word.get(x, \"NONE\"), x_test[i]))) for i in range(10)])\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khMZ7IAnzkg-"
      },
      "source": [
        "index = 1\r\n",
        "shap.force_plot(explainer.expected_value[0], shap_values[0][index], x_test_words[index], matplotlib=True), df_test[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPBBVMFzzwD2"
      },
      "source": [
        "shap_values_pool = shap_values[0]\r\n",
        "n_clusters = 20\r\n",
        "kmeans = KMeans(n_clusters= n_clusters, n_jobs=-1, max_iter=600)\r\n",
        "kmeans.fit(shap_values_pool)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdS4wzfZbwHW"
      },
      "source": [
        "homogeneity_score( y_test[:1000], kmeans.labels_), v_measure_score(y_test[:1000], kmeans.labels_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0DURnPPb7ZG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s164255\\anaconda3\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning:\n",
      "\n",
      "urllib3 (1.26.3) or chardet (4.0.0) doesn't match a supported version!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, homogeneity_score, v_measure_score, completeness_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import chart_studio.plotly as py\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5233, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>as a woman you should not complain about cleaning up your house as a man you should always take the garbage out</td>\n",
       "      <td>as a woman you should not complain about cleaning up your house as a man you should always take the garbage out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>momma said no cats inside my doghouse</td>\n",
       "      <td>momma said no cats inside my doghouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>simply addicted to guys hot scally lad</td>\n",
       "      <td>simply addicted to guys hot scally lad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>and hot soles</td>\n",
       "      <td>and hot soles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>lemmie eat a oreo do these dishes one oreo</td>\n",
       "      <td>lemmie eat a oreo do these dishes one oreo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>why the eggplant emoji would he say she looked like scream</td>\n",
       "      <td>why the eggplant emoji would he say she looked like scream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>baby monkey bathtime this is so adorable</td>\n",
       "      <td>baby monkey bathtime this is so adorable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>baseball season for the win yankees this is where the love started</td>\n",
       "      <td>baseball season for the win yankees this is where the love started</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>i am an early bird and i am a night owl so i am wise and have worms</td>\n",
       "      <td>i am an early bird and i am a night owl so i am wise and have worms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>overdosing on heavy drugs does not sound bad tonight i do that every day</td>\n",
       "      <td>overdosing on heavy drugs does not sound bad tonight i do that every day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  label  \\\n",
       "0      1      0   \n",
       "1      2      0   \n",
       "2      3      0   \n",
       "3      4      0   \n",
       "4      5      0   \n",
       "5      6      0   \n",
       "6      7      0   \n",
       "7      8      0   \n",
       "8      9      0   \n",
       "9     10      0   \n",
       "\n",
       "                                                                                                                text  \\\n",
       "0   as a woman you should not complain about cleaning up your house as a man you should always take the garbage out    \n",
       "1                                                                             momma said no cats inside my doghouse    \n",
       "2                                                                             simply addicted to guys hot scally lad   \n",
       "3                                                                                                      and hot soles   \n",
       "4                                                                        lemmie eat a oreo do these dishes one oreo    \n",
       "5                                                        why the eggplant emoji would he say she looked like scream    \n",
       "6                                                                          baby monkey bathtime this is so adorable    \n",
       "7                                                 baseball season for the win yankees this is where the love started   \n",
       "8                                               i am an early bird and i am a night owl so i am wise and have worms    \n",
       "9                                          overdosing on heavy drugs does not sound bad tonight i do that every day    \n",
       "\n",
       "                                                                                                           processed  \n",
       "0   as a woman you should not complain about cleaning up your house as a man you should always take the garbage out   \n",
       "1                                                                             momma said no cats inside my doghouse   \n",
       "2                                                                             simply addicted to guys hot scally lad  \n",
       "3                                                                                                      and hot soles  \n",
       "4                                                                        lemmie eat a oreo do these dishes one oreo   \n",
       "5                                                        why the eggplant emoji would he say she looked like scream   \n",
       "6                                                                          baby monkey bathtime this is so adorable   \n",
       "7                                                 baseball season for the win yankees this is where the love started  \n",
       "8                                               i am an early bird and i am a night owl so i am wise and have worms   \n",
       "9                                          overdosing on heavy drugs does not sound bad tonight i do that every day   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/davidson_dataset.csv')\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2badab061d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"label\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['text'].values\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word representation\n",
    "We are going to use TF*IDF (Term frequency- Inverse document frequency) vectorizer to convert our text into numbers.\n",
    "\n",
    "TF = Frequency of term\n",
    "\n",
    "IDF = No of docs/ No of docs with the term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5233, 5000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid = TfidfVectorizer(max_features=5000)\n",
    "x = tfid.fit_transform(x).toarray()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ability',\n",
       " 'able',\n",
       " 'abortion',\n",
       " 'about',\n",
       " 'above',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absurd',\n",
       " 'abu',\n",
       " 'abuse',\n",
       " 'accent',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'accident',\n",
       " 'accidentally',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accounts',\n",
       " 'accurate',\n",
       " 'ace',\n",
       " 'acquire',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'active',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'adam',\n",
       " 'add',\n",
       " 'added',\n",
       " 'address',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'adopting',\n",
       " 'adorable',\n",
       " 'adult',\n",
       " 'advanced',\n",
       " 'advice',\n",
       " 'afc',\n",
       " 'afford',\n",
       " 'afghanistan',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'age',\n",
       " 'agenda',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'aid',\n",
       " 'aids',\n",
       " 'aim',\n",
       " 'air',\n",
       " 'airlines',\n",
       " 'airs',\n",
       " 'aka',\n",
       " 'al',\n",
       " 'alabama',\n",
       " 'alarm',\n",
       " 'albino',\n",
       " 'album',\n",
       " 'alcohol',\n",
       " 'alcoholism',\n",
       " 'alex',\n",
       " 'alfredo',\n",
       " 'alien',\n",
       " 'aliens',\n",
       " 'alike',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allah',\n",
       " 'allied',\n",
       " 'allowed',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'aluminum',\n",
       " 'alves',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amazing',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'amissing',\n",
       " 'amnesty',\n",
       " 'among',\n",
       " 'amos',\n",
       " 'amove',\n",
       " 'an',\n",
       " 'anaconda',\n",
       " 'anchor',\n",
       " 'ancient',\n",
       " 'and',\n",
       " 'anderson',\n",
       " 'andy',\n",
       " 'angel',\n",
       " 'angeles',\n",
       " 'angelou',\n",
       " 'anger',\n",
       " 'anglo',\n",
       " 'angry',\n",
       " 'angus',\n",
       " 'animal',\n",
       " 'animals',\n",
       " 'ankles',\n",
       " 'ann',\n",
       " 'anniversary',\n",
       " 'announced',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'anonymous',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answers',\n",
       " 'ant',\n",
       " 'anthem',\n",
       " 'anthony',\n",
       " 'anti',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'ape',\n",
       " 'apes',\n",
       " 'apologize',\n",
       " 'app',\n",
       " 'apparently',\n",
       " 'appear',\n",
       " 'apple',\n",
       " 'appointee',\n",
       " 'appreciate',\n",
       " 'approaching',\n",
       " 'april',\n",
       " 'arab',\n",
       " 'arabia',\n",
       " 'arabic',\n",
       " 'are',\n",
       " 'area',\n",
       " 'areally',\n",
       " 'aremember',\n",
       " 'arent',\n",
       " 'argument',\n",
       " 'arizona',\n",
       " 'arm',\n",
       " 'armed',\n",
       " 'armor',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'around',\n",
       " 'arrested',\n",
       " 'arrived',\n",
       " 'arrogant',\n",
       " 'arrow',\n",
       " 'art',\n",
       " 'article',\n",
       " 'artist',\n",
       " 'artists',\n",
       " 'aryan',\n",
       " 'as',\n",
       " 'ascii',\n",
       " 'ashamed',\n",
       " 'ashanti',\n",
       " 'ashton',\n",
       " 'ashy',\n",
       " 'asian',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'ass',\n",
       " 'assault',\n",
       " 'asshole',\n",
       " 'assholes',\n",
       " 'assume',\n",
       " 'assuming',\n",
       " 'assumptions',\n",
       " 'astros',\n",
       " 'at',\n",
       " 'ate',\n",
       " 'atleast',\n",
       " 'attached',\n",
       " 'attack',\n",
       " 'attacked',\n",
       " 'attacks',\n",
       " 'attention',\n",
       " 'attitude',\n",
       " 'attitudes',\n",
       " 'attorney',\n",
       " 'attract',\n",
       " 'august',\n",
       " 'aunt',\n",
       " 'aurora',\n",
       " 'austin',\n",
       " 'authentic',\n",
       " 'auto',\n",
       " 'autocorrect',\n",
       " 'automatically',\n",
       " 'aux',\n",
       " 'available',\n",
       " 'average',\n",
       " 'avi',\n",
       " 'awakening',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awkward',\n",
       " 'aye',\n",
       " 'babe',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'background',\n",
       " 'backpack',\n",
       " 'backs',\n",
       " 'backwards',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'baddest',\n",
       " 'badge',\n",
       " 'badly',\n",
       " 'bae',\n",
       " 'bag',\n",
       " 'bags',\n",
       " 'bait',\n",
       " 'bake',\n",
       " 'baked',\n",
       " 'baker',\n",
       " 'balcony',\n",
       " 'bald',\n",
       " 'ball',\n",
       " 'ballot',\n",
       " 'ballots',\n",
       " 'balls',\n",
       " 'bama',\n",
       " 'bambino',\n",
       " 'ban',\n",
       " 'banana',\n",
       " 'band',\n",
       " 'bands',\n",
       " 'bandwagon',\n",
       " 'bank',\n",
       " 'banking',\n",
       " 'bankrupted',\n",
       " 'banned',\n",
       " 'banning',\n",
       " 'bar',\n",
       " 'barack',\n",
       " 'barely',\n",
       " 'bargain',\n",
       " 'barkley',\n",
       " 'barrel',\n",
       " 'barry',\n",
       " 'barrybonds',\n",
       " 'bars',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'bases',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basis',\n",
       " 'basketball',\n",
       " 'bastard',\n",
       " 'bat',\n",
       " 'batch',\n",
       " 'bath',\n",
       " 'bathing',\n",
       " 'bathrobes',\n",
       " 'bats',\n",
       " 'battle',\n",
       " 'bbc',\n",
       " 'bc',\n",
       " 'be',\n",
       " 'bean',\n",
       " 'beaner',\n",
       " 'beaners',\n",
       " 'beanies',\n",
       " 'beans',\n",
       " 'bear',\n",
       " 'beard',\n",
       " 'beast',\n",
       " 'beat',\n",
       " 'beating',\n",
       " 'beats',\n",
       " 'beautiful',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'bed',\n",
       " 'bee',\n",
       " 'been',\n",
       " 'beer',\n",
       " 'bees',\n",
       " 'before',\n",
       " 'beginning',\n",
       " 'behind',\n",
       " 'beiber',\n",
       " 'bein',\n",
       " 'being',\n",
       " 'beliefs',\n",
       " 'believe',\n",
       " 'bell',\n",
       " 'belly',\n",
       " 'belong',\n",
       " 'belongs',\n",
       " 'below',\n",
       " 'belt',\n",
       " 'beltran',\n",
       " 'bend',\n",
       " 'bengals',\n",
       " 'benghazi',\n",
       " 'benz',\n",
       " 'berk',\n",
       " 'berlin',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'better',\n",
       " 'betting',\n",
       " 'between',\n",
       " 'beverly',\n",
       " 'beyonc',\n",
       " 'bf',\n",
       " 'biden',\n",
       " 'bieber',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bigotry',\n",
       " 'bike',\n",
       " 'bill',\n",
       " 'bills',\n",
       " 'billy',\n",
       " 'bin',\n",
       " 'bird',\n",
       " 'birdhouse',\n",
       " 'birds',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bitches',\n",
       " 'bite',\n",
       " 'bites',\n",
       " 'bitten',\n",
       " 'bitter',\n",
       " 'bizarre',\n",
       " 'black',\n",
       " 'blackish',\n",
       " 'blacks',\n",
       " 'blame',\n",
       " 'blanket',\n",
       " 'blast',\n",
       " 'bless',\n",
       " 'blind',\n",
       " 'blk',\n",
       " 'block',\n",
       " 'blocked',\n",
       " 'blog',\n",
       " 'blonde',\n",
       " 'blood',\n",
       " 'bloody',\n",
       " 'blow',\n",
       " 'blows',\n",
       " 'blue',\n",
       " 'bluejays',\n",
       " 'blueprint',\n",
       " 'blues',\n",
       " 'blunt',\n",
       " 'bo',\n",
       " 'board',\n",
       " 'boat',\n",
       " 'bob',\n",
       " 'body',\n",
       " 'bomb',\n",
       " 'bombing',\n",
       " 'bombs',\n",
       " 'bon',\n",
       " 'bone',\n",
       " 'bones',\n",
       " 'boo',\n",
       " 'book',\n",
       " 'books',\n",
       " 'boon',\n",
       " 'boonies',\n",
       " 'boot',\n",
       " 'boots',\n",
       " 'border',\n",
       " 'boring',\n",
       " 'born',\n",
       " 'bosh',\n",
       " 'boss',\n",
       " 'boston',\n",
       " 'both',\n",
       " 'bother',\n",
       " 'bottle',\n",
       " 'bottles',\n",
       " 'bottom',\n",
       " 'bought',\n",
       " 'bounce',\n",
       " 'bound',\n",
       " 'boutta',\n",
       " 'bowl',\n",
       " 'box',\n",
       " 'boxes',\n",
       " 'boxing',\n",
       " 'boy',\n",
       " 'boyden',\n",
       " 'boyfriend',\n",
       " 'boys',\n",
       " 'bra',\n",
       " 'brady',\n",
       " 'braids',\n",
       " 'brain',\n",
       " 'brainer',\n",
       " 'brakes',\n",
       " 'brand',\n",
       " 'brandon',\n",
       " 'braves',\n",
       " 'bravo',\n",
       " 'brazil',\n",
       " 'bread',\n",
       " 'break',\n",
       " 'breakfast',\n",
       " 'breaking',\n",
       " 'breaks',\n",
       " 'breath',\n",
       " 'breed',\n",
       " 'breeds',\n",
       " 'brett',\n",
       " 'brian',\n",
       " 'bridge',\n",
       " 'bright',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'british',\n",
       " 'bro',\n",
       " 'broad',\n",
       " 'broadcast',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'broner',\n",
       " 'bronx',\n",
       " 'brooks',\n",
       " 'bros',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'brought',\n",
       " 'broward',\n",
       " 'brown',\n",
       " 'brownie',\n",
       " 'brownies',\n",
       " 'browns',\n",
       " 'bruins',\n",
       " 'bruised',\n",
       " 'bruises',\n",
       " 'brunch',\n",
       " 'bryan',\n",
       " 'btown',\n",
       " 'btw',\n",
       " 'bubble',\n",
       " 'bubbles',\n",
       " 'buck',\n",
       " 'bucket',\n",
       " 'bud',\n",
       " 'buddy',\n",
       " 'bugs',\n",
       " 'building',\n",
       " 'built',\n",
       " 'bull',\n",
       " 'bullets',\n",
       " 'bulls',\n",
       " 'bullshit',\n",
       " 'bum',\n",
       " 'bumble',\n",
       " 'bumblebee',\n",
       " 'bumblebees',\n",
       " 'bumped',\n",
       " 'bun',\n",
       " 'bunch',\n",
       " 'bunny',\n",
       " 'buns',\n",
       " 'burger',\n",
       " 'burn',\n",
       " 'burner',\n",
       " 'burnt',\n",
       " 'bus',\n",
       " 'bush',\n",
       " 'business',\n",
       " 'bust',\n",
       " 'but',\n",
       " 'butt',\n",
       " 'butter',\n",
       " 'button',\n",
       " 'butts',\n",
       " 'butyou',\n",
       " 'buy',\n",
       " 'buying',\n",
       " 'buys',\n",
       " 'by',\n",
       " 'bye',\n",
       " 'cabinet',\n",
       " 'cactus',\n",
       " 'cage',\n",
       " 'caged',\n",
       " 'cake',\n",
       " 'calamity',\n",
       " 'california',\n",
       " 'call',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'callingg',\n",
       " 'calls',\n",
       " 'calm',\n",
       " 'cam',\n",
       " 'came',\n",
       " 'camel',\n",
       " 'camels',\n",
       " 'camera',\n",
       " 'camp',\n",
       " 'campaign',\n",
       " 'campus',\n",
       " 'can',\n",
       " 'canada',\n",
       " 'canadian',\n",
       " 'cancel',\n",
       " 'cancer',\n",
       " 'candidate',\n",
       " 'candle',\n",
       " 'candy',\n",
       " 'cannot',\n",
       " 'cano',\n",
       " 'cans',\n",
       " 'cant',\n",
       " 'cap',\n",
       " 'capri',\n",
       " 'captain',\n",
       " 'captured',\n",
       " 'car',\n",
       " 'card',\n",
       " 'cardinals',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'career',\n",
       " 'careless',\n",
       " 'cares',\n",
       " 'cargos',\n",
       " 'carolina',\n",
       " 'carpet',\n",
       " 'carried',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'cars',\n",
       " 'cartel',\n",
       " 'carter',\n",
       " 'cartoon',\n",
       " 'case',\n",
       " 'casey',\n",
       " 'cash',\n",
       " 'cashman',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'catchers',\n",
       " 'catches',\n",
       " 'catching',\n",
       " 'catering',\n",
       " 'catholic',\n",
       " 'catholics',\n",
       " 'cats',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'causing',\n",
       " 'cavs',\n",
       " 'cease',\n",
       " 'celebrate',\n",
       " 'celebrated',\n",
       " 'celebrating',\n",
       " 'celestial',\n",
       " 'celtic',\n",
       " 'celtics',\n",
       " 'center',\n",
       " 'central',\n",
       " 'certainly',\n",
       " 'chaber',\n",
       " 'chainsaws',\n",
       " 'challenge',\n",
       " 'challenger',\n",
       " 'champagne',\n",
       " 'champion',\n",
       " 'champions',\n",
       " 'chance',\n",
       " 'chances',\n",
       " 'chanel',\n",
       " 'change',\n",
       " 'changes',\n",
       " 'changing',\n",
       " 'channel',\n",
       " 'chaplin',\n",
       " 'character',\n",
       " 'charge',\n",
       " 'charges',\n",
       " 'charity',\n",
       " 'charles',\n",
       " 'charlie',\n",
       " 'charlies',\n",
       " 'charlotte',\n",
       " 'chase',\n",
       " 'chasing',\n",
       " 'chat',\n",
       " 'chatter',\n",
       " 'chava',\n",
       " 'cheaper',\n",
       " 'cheat',\n",
       " 'cheated',\n",
       " 'cheating',\n",
       " 'cheats',\n",
       " 'check',\n",
       " 'checking',\n",
       " 'cheeks',\n",
       " 'cheeky',\n",
       " 'cheer',\n",
       " 'cheers',\n",
       " 'cheese',\n",
       " 'cheesecake',\n",
       " 'cheesy',\n",
       " 'cheez',\n",
       " 'cherry',\n",
       " 'chest',\n",
       " 'chicago',\n",
       " 'chick',\n",
       " 'chicken',\n",
       " 'chicks',\n",
       " 'child',\n",
       " 'childish',\n",
       " 'children',\n",
       " 'chili',\n",
       " 'chill',\n",
       " 'chilling',\n",
       " 'chimps',\n",
       " 'chin',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'chink',\n",
       " 'chinks',\n",
       " 'chip',\n",
       " 'chipotle',\n",
       " 'chips',\n",
       " 'chirpin',\n",
       " 'chocolate',\n",
       " 'choice',\n",
       " 'choke',\n",
       " 'choked',\n",
       " 'choose',\n",
       " 'chops',\n",
       " 'chose',\n",
       " 'chosen',\n",
       " 'chowder',\n",
       " 'chris',\n",
       " 'christ',\n",
       " 'christian',\n",
       " 'christians',\n",
       " 'christmas',\n",
       " 'chuck',\n",
       " 'chucker',\n",
       " 'chug',\n",
       " 'chugs',\n",
       " 'chulo',\n",
       " 'chunk',\n",
       " 'chunky',\n",
       " 'church',\n",
       " 'cigarette',\n",
       " 'circle',\n",
       " 'circles',\n",
       " 'circulating',\n",
       " 'citizen',\n",
       " 'citizens',\n",
       " 'city',\n",
       " 'civil',\n",
       " 'claim',\n",
       " 'claiming',\n",
       " 'clam',\n",
       " 'clams',\n",
       " 'clan',\n",
       " 'clap',\n",
       " 'class',\n",
       " 'classic',\n",
       " 'classics',\n",
       " 'cle',\n",
       " 'clean',\n",
       " 'cleaned',\n",
       " 'cleaning',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'cleveland',\n",
       " 'clever',\n",
       " 'cliff',\n",
       " 'climate',\n",
       " 'climb',\n",
       " 'clip',\n",
       " 'clippers',\n",
       " 'clips',\n",
       " 'clock',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closely',\n",
       " 'closer',\n",
       " 'closet',\n",
       " 'clothes',\n",
       " 'cloud',\n",
       " 'clowney',\n",
       " 'club',\n",
       " 'clubs',\n",
       " 'clutch',\n",
       " 'cnn',\n",
       " 'coach',\n",
       " 'coakley',\n",
       " 'coast',\n",
       " 'cocaine',\n",
       " 'cock',\n",
       " 'cocktail',\n",
       " 'coconut',\n",
       " 'cod',\n",
       " 'code',\n",
       " 'codeword',\n",
       " 'cody',\n",
       " 'coffee',\n",
       " 'coke',\n",
       " 'cokies',\n",
       " 'cold',\n",
       " 'colds',\n",
       " 'cole',\n",
       " 'collector',\n",
       " 'college',\n",
       " 'color',\n",
       " 'colorado',\n",
       " 'colored',\n",
       " 'colors',\n",
       " 'coloured',\n",
       " 'columbus',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comedian',\n",
       " 'comedy',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'commander',\n",
       " 'commence',\n",
       " 'comment',\n",
       " 'comments',\n",
       " 'commercial',\n",
       " 'committed',\n",
       " 'common',\n",
       " 'community',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'compared',\n",
       " 'comparison',\n",
       " 'complain',\n",
       " 'complaining',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'compliments',\n",
       " 'computer',\n",
       " 'con',\n",
       " 'conan',\n",
       " 'concede',\n",
       " 'concert',\n",
       " 'condoms',\n",
       " 'conference',\n",
       " 'confirmed',\n",
       " 'confused',\n",
       " 'congrats',\n",
       " 'congratulations',\n",
       " 'congress',\n",
       " 'consensus',\n",
       " 'conservative',\n",
       " 'conservatives',\n",
       " 'consider',\n",
       " 'consideration',\n",
       " 'considered',\n",
       " 'constantly',\n",
       " 'constitution',\n",
       " 'contact',\n",
       " 'contacts',\n",
       " 'contain',\n",
       " 'continue',\n",
       " 'continues',\n",
       " 'contract',\n",
       " 'control',\n",
       " 'conversation',\n",
       " 'convinced',\n",
       " 'convo',\n",
       " 'cook',\n",
       " 'cookie',\n",
       " 'cookies',\n",
       " 'cooking',\n",
       " 'cool',\n",
       " 'coolatta',\n",
       " 'cooler',\n",
       " 'coolest',\n",
       " 'coolie',\n",
       " 'coon',\n",
       " 'coons',\n",
       " 'coors',\n",
       " 'cop',\n",
       " 'cops',\n",
       " 'copy',\n",
       " 'cord',\n",
       " 'core',\n",
       " 'cork',\n",
       " 'corn',\n",
       " 'corner',\n",
       " 'corporate',\n",
       " 'correct',\n",
       " 'cosby',\n",
       " 'costs',\n",
       " 'costume',\n",
       " 'cotton',\n",
       " 'couch',\n",
       " 'could',\n",
       " 'count',\n",
       " 'counting',\n",
       " 'countries',\n",
       " 'country',\n",
       " 'counts',\n",
       " 'county',\n",
       " 'coupe',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'court',\n",
       " 'cousin',\n",
       " 'cousins',\n",
       " 'cover',\n",
       " 'coverage',\n",
       " 'covered',\n",
       " 'cow',\n",
       " 'coward',\n",
       " 'cowboy',\n",
       " 'cowboys',\n",
       " 'crack',\n",
       " 'cracked',\n",
       " 'cracker',\n",
       " 'crackers',\n",
       " 'crash',\n",
       " 'craving',\n",
       " 'crayons',\n",
       " 'crazy',\n",
       " 'cream',\n",
       " 'create',\n",
       " 'creatures',\n",
       " 'credentials',\n",
       " 'credit',\n",
       " 'creep',\n",
       " 'creepy',\n",
       " 'crew',\n",
       " 'crib',\n",
       " 'cried',\n",
       " 'crime',\n",
       " 'criminal',\n",
       " 'criminals',\n",
       " 'cripple',\n",
       " 'cripples',\n",
       " 'crisco',\n",
       " 'crisis',\n",
       " 'crist',\n",
       " 'critical',\n",
       " 'crochet',\n",
       " 'cross',\n",
       " 'crossed',\n",
       " 'crossing',\n",
       " 'crow',\n",
       " 'crowd',\n",
       " 'crown',\n",
       " 'crows',\n",
       " 'cruise',\n",
       " 'crush',\n",
       " 'crushed',\n",
       " 'crust',\n",
       " 'cruz',\n",
       " 'cry',\n",
       " 'crybaby',\n",
       " 'crying',\n",
       " 'crystal',\n",
       " 'cubs',\n",
       " 'cuddle',\n",
       " 'cue',\n",
       " 'culture',\n",
       " 'cunt',\n",
       " 'cunts',\n",
       " 'cup',\n",
       " 'curious',\n",
       " 'curly',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'curriculum',\n",
       " 'curved',\n",
       " 'cushite',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'cutest',\n",
       " 'cuts',\n",
       " 'cutting',\n",
       " 'dad',\n",
       " 'daddy',\n",
       " 'daily',\n",
       " 'damage',\n",
       " 'damn',\n",
       " 'dance',\n",
       " 'dani',\n",
       " 'daniel',\n",
       " 'daniels',\n",
       " 'danny',\n",
       " 'dare',\n",
       " 'dark',\n",
       " 'darkie',\n",
       " 'darkness',\n",
       " 'darling',\n",
       " 'date',\n",
       " 'daughter',\n",
       " 'daughters',\n",
       " 'david',\n",
       " 'davis',\n",
       " 'dawn',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dc',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'dear',\n",
       " 'death',\n",
       " 'debate',\n",
       " 'debating',\n",
       " 'decaf',\n",
       " 'december',\n",
       " 'decent',\n",
       " 'decide',\n",
       " 'decided',\n",
       " 'decision',\n",
       " 'decisions',\n",
       " 'declared',\n",
       " 'deemed',\n",
       " 'deep',\n",
       " 'deer',\n",
       " 'defeat',\n",
       " 'defeating',\n",
       " 'defend',\n",
       " 'defending',\n",
       " 'defense',\n",
       " 'defiantly',\n",
       " 'definitely',\n",
       " 'definition',\n",
       " 'deflect',\n",
       " 'degenerate',\n",
       " 'delete',\n",
       " 'deleted',\n",
       " 'delicious',\n",
       " 'delusional',\n",
       " 'demand',\n",
       " 'democracy',\n",
       " 'democrat',\n",
       " 'democratic',\n",
       " 'democrats',\n",
       " 'den',\n",
       " 'denver',\n",
       " 'deny',\n",
       " 'department',\n",
       " 'deported',\n",
       " 'depth',\n",
       " 'derek',\n",
       " 'descended',\n",
       " 'describe',\n",
       " 'describes',\n",
       " 'desert',\n",
       " 'deserve',\n",
       " 'deserved',\n",
       " 'deserves',\n",
       " 'designer',\n",
       " 'desperately',\n",
       " 'despite',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data\n",
    "  We split data into training, test and pool. Pool is the unlabelled pool we want to generate SHAP clusters for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "indices =  np.random.randint(low=0, high=x.shape[0], size=x.shape[0])\n",
    "train_indices = indices[0:round(0.7*x.shape[0])]\n",
    "test_indices = indices[round(0.7*x.shape[0]): round(0.8*x.shape[0])]\n",
    "pool_indices = indices[round(0.8*x.shape[0]):]\n",
    "df_train = df.iloc[train_indices]['text'].values\n",
    "df_test = df.iloc[test_indices]['text'].values\n",
    "df_pool = df.iloc[pool_indices]['text'].values\n",
    "x_train = x[train_indices]\n",
    "y_train = y[train_indices]\n",
    "x_test = x[test_indices]\n",
    "y_test = y[test_indices]\n",
    "x_pool = x[pool_indices]\n",
    "y_pool = y[pool_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model fitting\n",
    "We do a simple grid search to find the best SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_iter=1000\n",
    "C= [0.5, 1, 2]\n",
    "best_f1 = 0\n",
    "model = None\n",
    "for c in C:\n",
    "    m = SVC( max_iter=max_iter, C=c, kernel='linear', class_weight='balanced', probability=True)\n",
    "    m.fit(x_train, y_train)\n",
    "    m.score(x_train,y_train)\n",
    "    predictions = m.predict(x_test)\n",
    "    f1 = f1_score(predictions, y_test)\n",
    "    if  f1 > best_f1:\n",
    "        model = m\n",
    "        best_f1 = f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure the f1_score and plot confusion matrix. Recall:\n",
    "- f1 score = 2PR/ (P+R)\n",
    "- Precision = actual positives/ predicted positives\n",
    "- Recall = predicted positives/ total actual positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_pool)\n",
    "f1_score(predictions, y_pool), accuracy_score(y_pool, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert predict probability to uncertainty. In binary classification this would be the same as 1-P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classwise_uncertainty = model.predict_proba(x_pool)\n",
    "uncertainty = 1 - np.max(classwise_uncertainty, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(uncertainty)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_pool, predictions),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain model using SHAP\n",
    "\n",
    "Refer https://christophm.github.io/interpretable-ml-book/shap.html\n",
    "\n",
    "The goal of SHAP is to explain the prediction of an instance x by computing the contribution of each feature to the prediction. The SHAP explanation method computes Shapley values from coalitional game theory. The feature values of a data instance act as players in a coalition. Shapley values tell us how to fairly distribute the “payout” (= the prediction) among the features\n",
    "\n",
    "We call the SHAP explainer for linear models\n",
    "shapely values produced have same dimensions as data passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.LinearExplainer(model, x_train,feature_dependence=\"independent\")\n",
    "shap_values_train = explainer.shap_values(x_train)\n",
    "shap_values_pool = explainer.shap_values(x_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_pool.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain a single positive prediction at 'index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postive_index = 0\n",
    "index = np.where(predictions==1)[0][postive_index]\n",
    "print(\"text \", df_test[index], \" prediction: \", predictions[index], \"actual \", y_test[index])\n",
    "shap.force_plot(explainer.expected_value, \n",
    "                               shap_values_pool[index,:], \n",
    "                               x_test[index,:], feature_names = tfid.get_feature_names(),\n",
    "               matplotlib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the overall feature importance using summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_pool, x_pool, feature_names=tfid.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_pool.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering data\n",
    "We are going to cluster the training data using SHAP explanations (shapely space)\n",
    "SHAP clustering works by clustering on Shapley values of each instance. \n",
    "This means that you cluster instances by explanation similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 50\n",
    "kmeans = KMeans(n_clusters= n_clusters, n_jobs=-1, max_iter=600)\n",
    "kmeans.fit(shap_values_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homogeneity_score( y_pool, kmeans.labels_), v_measure_score(y_pool, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use cosine distance instead of euclidean distance to measure the similarity between the documents.\n",
    "As the size of the document increases, the number of common words (euclidean) tend to increase \n",
    "even if the documents talk about different topics. The cosine similarity helps overcome this fundamental flaw \n",
    "and finds the similarity irrespective of size.\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/cosine-similarity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similarity of each point in cluster to its centroid\n",
    "similarity_to_center = []\n",
    "for i, instance in enumerate(shap_values_pool):\n",
    "    cluster_label = kmeans.labels_[i] # cluster of this instance\n",
    "    centroid = kmeans.cluster_centers_[cluster_label] # cluster center of the cluster of that instance\n",
    "    similarity = 1-cosine(instance, centroid) # 1- cosine distance gives similarity\n",
    "    similarity_to_center.append(similarity)\n",
    "    \n",
    "centroid_match = [None]*n_clusters\n",
    "centroid_indices =[None]*n_clusters\n",
    "for i, instance in enumerate(shap_values_pool):\n",
    "    cluster_label = kmeans.labels_[i]     \n",
    "    if centroid_match[cluster_label] is None or similarity_to_center[i] > centroid_match[cluster_label]:\n",
    "        centroid_indices[cluster_label] = i\n",
    "        centroid_match[cluster_label] = similarity_to_center[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, perplexity=20)\n",
    "principals = tsne.fit_transform(shap_values_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "colorscale = [[0, 'mediumturquoise'], [1, 'salmon']]\n",
    "collect = dict()\n",
    "sizes = []\n",
    "color = ['hsl(' + str(h) + ',80%' + ',50%)' for h in np.linspace(0, 255, n_clusters)]\n",
    "df_final_labels = pd.DataFrame()\n",
    "for cluster_id in np.unique(kmeans.labels_):\n",
    "    cluster_indices = np.where(kmeans.labels_ == cluster_id)    \n",
    "    \n",
    "    cluster_text = df_pool[cluster_indices]\n",
    "    center_index = centroid_indices[cluster_id]\n",
    "    center_text = df_pool[center_index]\n",
    "    sizes.append(len(cluster_indices[0]))\n",
    "    df_cluster = pd.DataFrame({'text': cluster_text})\n",
    "    df_cluster['cluster_id'] = cluster_id\n",
    "    df_cluster['centroid'] = False\n",
    "    df_cluster = df_cluster.append({'text':center_text, 'cluster_id':cluster_id,\n",
    "                                    'centroid':True }, ignore_index=True)\n",
    "    df_final_labels = pd.concat([df_final_labels, df_cluster])\n",
    "\n",
    "    cp = principals[cluster_indices]\n",
    "    data.append(go.Heatmap(x=cp[:, 0],\n",
    "                           y=cp[:, 1],\n",
    "                           z=uncertainty[cluster_indices],\n",
    "                           name='uncertainity map',\n",
    "                           visible=True,\n",
    "                           showscale=False,\n",
    "                           colorscale=colorscale,\n",
    "                                         ))\n",
    "    data.append(go.Scatter(x = cp[:,0],\n",
    "                   y = cp[:,1],\n",
    "                   mode='markers',                    \n",
    "                hovertext=cluster_text,\n",
    "                            marker=dict(color=color[cluster_id],\n",
    "                                                   size=10),\n",
    "                           name = 'cluster '+ str(cluster_id)\n",
    "                          ))\n",
    "    data.append(go.Scatter(x = [principals[center_index, 0]],\n",
    "                   y = [principals[center_index, 1]],\n",
    "                   mode='markers',  \n",
    "                           marker=dict(color=color[cluster_id],\n",
    "                                                   size=15,\n",
    "                                                   line=dict(color='black', width=5)),\n",
    "                           name = 'centroid cluster '+ str(cluster_id),\n",
    "                           visible='legendonly',\n",
    "                           \n",
    "                          ))\n",
    "    collect[cluster_id] = df_pool[cluster_indices]\n",
    "    \n",
    "fig = go.Figure(data=data)\n",
    "fig.show()\n",
    "df_final_labels.to_csv('df_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze uncertainty within clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "ranges = []\n",
    "for cluster_id in np.unique(kmeans.labels_):\n",
    "    cluster_indices = np.where(kmeans.labels_ == cluster_id)    \n",
    "    uncertainty_cluster = uncertainty[cluster_indices]\n",
    "    rng = np.max(uncertainty_cluster)- np.min(uncertainty_cluster)\n",
    "    print(cluster_id, \"range \", rng)\n",
    "    ranges.append(rng)\n",
    "    print(y_pool[cluster_indices])\n",
    "    print(\"\\n\")\n",
    "    data.append(go.Histogram(x=uncertainty_cluster, name=str(cluster_id), showlegend=True,visible='legendonly'))\n",
    "fig = go.Figure(data=data)\n",
    "url=py.plot(fig, filename='clusters_50', sharing='public')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_uncertain_indices = (-uncertainty).argsort()[:20]\n",
    "y_pool[max_uncertain_indices], uncertainty[max_uncertain_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_id = 8\n",
    "cluster_indices = np.where(kmeans.labels_ == cluster_id)    \n",
    "d = {'text' : df_pool[cluster_indices], 'uncertainty': uncertainty[cluster_indices], 'label': y_pool[cluster_indices]}\n",
    "\n",
    "pd.DataFrame(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=cluster_indices[0][10]\n",
    "shap.force_plot(explainer.expected_value, shap_values_pool[i,:], x_pool[i,:], feature_names = tfid.get_feature_names(),\n",
    "               matplotlib=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find optimal cluster size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homogeneity_scores = []\n",
    "v_measure_scores = []\n",
    "completeness_scores = []\n",
    "n_iters = 10\n",
    "ranges = list(range(10, 110, 10))\n",
    "for k in ranges:\n",
    "    vavg = 0\n",
    "    havg = 0\n",
    "    cavg = 0\n",
    "    for i in range(n_iters):\n",
    "        kmeans = KMeans(n_clusters= n_clusters, n_jobs=-1)\n",
    "        kmeans.fit(shap_values_pool)\n",
    "        v = v_measure_score(labels_pred=kmeans.labels_, labels_true=y_pool) \n",
    "        h = homogeneity_score(labels_pred=kmeans.labels_, labels_true=y_pool) \n",
    "        c = completeness_score(labels_pred=kmeans.labels_, labels_true=y_pool) \n",
    "        vavg += v\n",
    "        havg += h\n",
    "        cavg += c\n",
    "    homogeneity_scores.append(havg/n_iters)\n",
    "    v_measure_scores.append(vavg/n_iters)\n",
    "    completeness_scores.append(cavg/n_iters)\n",
    "    print(k, \"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [go.Scatter(x=ranges, y=homogeneity_scores, mode=\"lines\", name=\"homogeneity\"),\n",
    "        go.Scatter(x=ranges, y=v_measure_scores, mode=\"lines\", name=\"v_measure\"),\n",
    "        go.Scatter(x=ranges, y=completeness_scores, mode=\"lines\", name=\"completeness\")\n",
    "        ]\n",
    "fig = go.Figure(data=data)\n",
    "fig.update_layout(xaxis_title=\"no of clusters\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add centroid of each cluster to the training set and retrain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices_new = np.append(train_indices, centroid_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = SVC( max_iter=max_iter, C=C, kernel='linear')\n",
    "x_train_new = x[train_indices_new]\n",
    "y_train_new = y[train_indices_new]\n",
    "model1.fit(x_train_new, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.score(x_train_new, y_train_new), model1.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = model1.predict(x_test)\n",
    "f1_score(y_test, predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predictions1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add entire x_pool back to training instead of just centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices_full = np.append(train_indices, pool_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = SVC(max_iter=max_iter, C=C, kernel='linear')\n",
    "x_train_full = x[train_indices_full]\n",
    "y_train_full = y[train_indices_full]\n",
    "model2.fit(x_train_full, y_train_full)\n",
    "model2.score(x_train_full, y_train_full), model2.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = model2.predict(x_test)\n",
    "f1_score(y_test, predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model with 20% train \", f1_score(y_test, predictions))\n",
    "print(\"Model with 20% train + center \", f1_score(y_test, predictions1))\n",
    "print(\"Model with 20% train + 60% pool \", f1_score(y_test, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model with 20% train \", accuracy_score(y_test, predictions))\n",
    "print(\"Model with 20% train + center \", accuracy_score(y_test, predictions1))\n",
    "print(\"Model with 20% train + 60% pool \", accuracy_score(y_test, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
